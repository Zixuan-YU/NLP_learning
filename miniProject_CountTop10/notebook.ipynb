{"cells":[{"source":"![mobydick](mobydick.jpg)","metadata":{},"id":"b1309988-b429-4fb0-8c4c-193582dbec93","cell_type":"markdown"},{"source":"In this workspace, you'll scrape the novel Moby Dick from the website [Project Gutenberg](https://www.gutenberg.org/) (which contains a large corpus of books) using the Python `requests` package. You'll extract words from this web data using `BeautifulSoup` before analyzing the distribution of words using the Natural Language ToolKit (`nltk`) and `Counter`.\n\nThe Data Science pipeline you'll build in this workspace can be used to visualize the word frequency distributions of any novel you can find on Project Gutenberg.","metadata":{},"id":"611e416c-70e7-478a-a3c8-e54f3fdb4a7f","cell_type":"markdown"},{"source":"# Import and download packages\nimport requests\nfrom bs4 import BeautifulSoup\nimport nltk\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nnltk.download('stopwords')\n\n# Start coding here... \n","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"executionTime":52,"lastSuccessfullyExecutedCode":"# Import and download packages\nimport requests\nfrom bs4 import BeautifulSoup\nimport nltk\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nnltk.download('stopwords')\n\n# Start coding here... \n","executionCancelledAt":null,"lastExecutedAt":1726301239877,"lastExecutedByKernel":"a470b5dd-20dd-4b09-ae27-d895c191aece","lastScheduledRunId":null,"outputsMetadata":{"0":{"height":59,"type":"stream"}}},"id":"15b5f52f-fd9b-4f0e-9fcc-f7733022c7c0","cell_type":"code","execution_count":105,"outputs":[{"output_type":"stream","name":"stderr","text":"[nltk_data] Downloading package stopwords to /home/repl/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"},{"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{},"execution_count":105}]},{"source":"# request the designated URL\nr = requests.get('https://s3.amazonaws.com/assets.datacamp.com/production/project_147/datasets/2701-h.htm')\nr.encoding = 'utf-8'\nhtml = r.text\nprint(\"The status code is \", r.status_code)\nprint(\"\\n\")\n# create a BeautifulSoup object\nhtml_soup = BeautifulSoup(res.text, 'html.parser')\nprint(html_soup.title)\nprint(\"\\n\")","metadata":{"executionCancelledAt":null,"executionTime":5124,"lastExecutedAt":1726301245001,"lastExecutedByKernel":"a470b5dd-20dd-4b09-ae27-d895c191aece","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# request the designated URL\nr = requests.get('https://s3.amazonaws.com/assets.datacamp.com/production/project_147/datasets/2701-h.htm')\nr.encoding = 'utf-8'\nhtml = r.text\nprint(\"The status code is \", r.status_code)\nprint(\"\\n\")\n# create a BeautifulSoup object\nhtml_soup = BeautifulSoup(res.text, 'html.parser')\nprint(html_soup.title)\nprint(\"\\n\")","outputsMetadata":{"0":{"height":185,"type":"stream"}}},"cell_type":"code","id":"e681a412-16cf-4b9e-b8c6-98efceedfc57","outputs":[{"output_type":"stream","name":"stdout","text":"The status code is  200\n\n\n<title>\n      Moby Dick; Or the Whale, by Herman Melville\n    </title>\n\n\n"}],"execution_count":106},{"source":"\ntokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n# extract the full text\nmoby_text = html_soup.get_text()\ntokens = tokenizer.tokenize(moby_text)\nprint(tokens[:5])","metadata":{"executionCancelledAt":null,"executionTime":103,"lastExecutedAt":1726301245105,"lastExecutedByKernel":"a470b5dd-20dd-4b09-ae27-d895c191aece","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"\ntokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n# extract the full text\nmoby_text = html_soup.get_text()\ntokens = tokenizer.tokenize(moby_text)\nprint(tokens[:5])","outputsMetadata":{"0":{"height":38,"type":"stream"}}},"cell_type":"code","id":"2e302497-978d-45f3-b4bb-283071c4d4dc","outputs":[{"output_type":"stream","name":"stdout","text":"['Moby', 'Dick', 'Or', 'the', 'Whale']\n"}],"execution_count":107},{"source":"words = [t.lower() for t in tokens]\nstop_words = stopwords.words('english')\n# print(stop_words)\nwords_no_stop = [w for w in words if w not in stop_words]\nprint(words_no_stop[:5])","metadata":{"executionCancelledAt":null,"executionTime":409,"lastExecutedAt":1726301245514,"lastExecutedByKernel":"a470b5dd-20dd-4b09-ae27-d895c191aece","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"words = [t.lower() for t in tokens]\nstop_words = stopwords.words('english')\n# print(stop_words)\nwords_no_stop = [w for w in words if w not in stop_words]\nprint(words_no_stop[:5])","outputsMetadata":{"0":{"height":38,"type":"stream"}}},"cell_type":"code","id":"36306687-65e8-4c28-9052-837ce6e47d29","outputs":[{"output_type":"stream","name":"stdout","text":"['moby', 'dick', 'whale', 'herman', 'melville']\n"}],"execution_count":108},{"source":"count = Counter(words_no_stop)\nfor word, cnt in count.most_common(10):\n    print(f\"{word}: {cnt}\")\n\ntop_ten = word_counts.most_common(10)\nprint(top_ten)","metadata":{"executionCancelledAt":null,"executionTime":63,"lastExecutedAt":1726301245577,"lastExecutedByKernel":"a470b5dd-20dd-4b09-ae27-d895c191aece","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"count = Counter(words_no_stop)\nfor word, cnt in count.most_common(10):\n    print(f\"{word}: {cnt}\")\n\ntop_ten = word_counts.most_common(10)\nprint(top_ten)","outputsMetadata":{"0":{"height":269,"type":"stream"}}},"cell_type":"code","id":"5837e9f9-4144-41ad-bd5a-600cd2dfd463","outputs":[{"output_type":"stream","name":"stdout","text":"whale: 1246\none: 925\nlike: 647\nupon: 568\nman: 527\nship: 519\nahab: 517\nye: 473\nsea: 455\nold: 452\n[('whale', 1246), ('one', 925), ('like', 647), ('upon', 568), ('man', 527), ('ship', 519), ('ahab', 517), ('ye', 473), ('sea', 455), ('old', 452)]\n"}],"execution_count":109}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}